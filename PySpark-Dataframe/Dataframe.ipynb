{"cells":[{"cell_type":"code","source":["# Create RDD from textfile and then create DF from RDD directly \n\nlines = sc.textFile('/FileStore/tables/people.txt')\nlines.collect() # ['Michael, 29', 'Andy, 30', 'Justin, 19']\n#print(type(lines))\n#dfFromRDD1 = spark.createDataFrame(lines) \n## cannot convert txt file to RDD to DF directly because of output of the RDD is ['Michael, 29', 'Andy, 30', 'Justin, 19']\n## output of RDD is not a tuple or row RDD\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"679e2817-6d44-4e88-a23c-d16678ff14b1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c16b92f2-7d30-4857-9e72-e831a0a9ac3f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\nfrom pyspark.sql.types import*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"210a0b44-a1f3-484d-8bc7-9629e57b6873"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create RDD from textfile, convert RDD into row RDD without schema and then create DF from row RDD\n\nlines = sc.textFile('/FileStore/tables/people.txt') \n# lines.collect() # ['Michael, 29', 'Andy, 30', 'Justin, 19']\nparts = lines.map(lambda l: l.split(\",\")) \n# parts.collect() # [['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]\npeople = parts.map(lambda p: Row(p[0],int(p[1]))) # Row RDD without schema\npeople.collect() # [<Row('Michael', 29)>, <Row('Andy', 30)>, <Row('Justin', 19)>]\npeople_df = spark.createDataFrame(people) # convert RDD into DF without schema\npeople_df.printSchema()\npeople_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a6cb2fe-7f40-41ef-86c4-18d93a1ef8d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+-------+---+\n|     _1| _2|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+-------+---+\n|     _1| _2|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# passing schema to the row RDD(without schema)\npeople_df_withschema = people.toDF([\"name\",\"age\"])\npeople_df_withschema.show()\n#people_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c386c725-8d17-4ec5-98b4-59a9452a1aa8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+---+\n|   name|age|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+---+\n|   name|age|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Create RDD from textfile, convert RDD into row RDD without schema, create schema and then create DF using row RDD and schema\n\nlines = sc.textFile('/FileStore/tables/people.txt') \n# lines.collect() # ['Michael, 29', 'Andy, 30', 'Justin, 19']\nparts = lines.map(lambda l: l.split(\",\")) \n# parts.collect() # [['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]\npeople = parts.map(lambda p: Row(p[0],int(p[1]))) # Row RDD without schema\n# creating schema using structtype and structfield\nschema = StructType([StructField(\"name\", StringType(), False),\n                     StructField(\"age\", IntegerType(), False)])\n\n# creating schema using for loop with same datatype\n# schemaString = \"name age\"\n# schema = StructType[StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n\n# Create a DataFrame by applying the schema to the RDD and print the schema\npeople_df = sqlContext.createDataFrame(people, schema)\npeople_df.printSchema()\npeople_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34e47b56-0595-4409-a7b6-4917df05ae6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = false)\n |-- age: integer (nullable = false)\n\n+-------+---+\n|   name|age|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = false)\n |-- age: integer (nullable = false)\n\n+-------+---+\n|   name|age|\n+-------+---+\n|Michael| 29|\n|   Andy| 30|\n| Justin| 19|\n+-------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Create RDD from textfile, convert RDD into row RDD with schema and then create DF from row RDD\n\nlines = sc.textFile('/FileStore/tables/people.txt') \n# lines.collect() # ['Michael, 29', 'Andy, 30', 'Justin, 19']\nparts = lines.map(lambda l: l.split(\",\")) \n# parts.collect() # [['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]\npeople = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) # Row RDD with schema\npeople.collect() # [Row(name='Michael', age=29), Row(name='Andy', age=30), Row(name='Justin', age=19)]\npeople_df = spark.createDataFrame(people) # convert RDD into DF with schema\npeople_df.printSchema()\npeople_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6fda191-f59c-4900-9755-53f0b0874845"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[22]: [Row(name='Michael', age=29),\n Row(name='Andy', age=30),\n Row(name='Justin', age=19)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[22]: [Row(name='Michael', age=29),\n Row(name='Andy', age=30),\n Row(name='Justin', age=19)]"]}}],"execution_count":0},{"cell_type":"code","source":["#Reading the people,txt file directly into the spark data frame... \n# since there is no schema in the text file, data in the data frame is also in \"comma separated format\"into separate columns\ndf4 = spark.read.text('/FileStore/tables/people.txt')\ndf4.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbef3ba7-151e-43be-83d8-ef7b06672416"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+\n|      value|\n+-----------+\n|Michael, 29|\n|   Andy, 30|\n| Justin, 19|\n+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+\n|      value|\n+-----------+\n|Michael, 29|\n|   Andy, 30|\n| Justin, 19|\n+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Reading customers.json file by loading it into df.. json file used has schema along with data\ndf = spark.read.json(\"/FileStore/tables/customers.json\")\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2be44c2d-755c-4a7b-b31c-1b6b43277ff1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# write the data frame into a file location in databricks\ndf.write.json(\"/FileStore/tables/customers_output.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c59b352-7c02-43f2-8b27-f118b42d9ebc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# checking the json file written using the above command\nwritten_df = spark.read.json(\"/FileStore/tables/customers_output.json\")\nwritten_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8023cd49-7111-4348-9d3e-d742f9f1c24d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading customers.json file into spark data frame\ndf2 = spark.read.load(\"/FileStore/tables/customers.json\", format=\"json\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19a8eaec-ed29-4f47-9aa3-a03cdfca48dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Reading users.parquet file into spark data frame \ndf3 = spark.read.load(\"/FileStore/tables/users.parquet\", format=\"parquet\")\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ab7d5b5-3113-4883-adc7-6efa409c548f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Reading multiline json file into spark data frame\nmultiline_df = spark.read.option(\"multiline\",\"true\").json(\"/FileStore/tables/multiline.json\")\nmultiline_df.show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1174ce80-97c4-4f7f-95e1-a5a1df8bb70d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------+------------+-----+-----------+-------+\n|               City|RecordNumber|State|ZipCodeType|Zipcode|\n+-------------------+------------+-----+-----------+-------+\n|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n+-------------------+------------+-----+-----------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------+------------+-----+-----------+-------+\n|               City|RecordNumber|State|ZipCodeType|Zipcode|\n+-------------------+------------+-----+-----------+-------+\n|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n+-------------------+------------+-----+-----------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Read multiple files \nmultiplefile_df = spark.read.json(['/FileStore/tables/customers.json','/FileStore/tables/customers_output.json'])\nmultiplefile_df.show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51ceb55f-6849-42b0-98b4-0f8b8aac11b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n+--------------------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Read multiple files with same schema\nmultiplefile_df = spark.read.json(['/FileStore/tables/customers.json','/FileStore/tables/customers_output.json'])\nmultiplefile_df.show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb6f27d1-bc99-43d5-895f-88dc4c317a41"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_new = spark.read.json(\"/FileStore/tables/customers_new.json\")\ndf_new.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92ea93d7-833d-4950-827e-55af4a0d3d48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+\n|             address|first_name|\n+--------------------+----------+\n|{New Orleans, LA,...|     James|\n|{Brighton, MI, 4 ...| Josephine|\n|{Bridgeport, NJ, ...|       Art|\n+--------------------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+\n|             address|first_name|\n+--------------------+----------+\n|{New Orleans, LA,...|     James|\n|{Brighton, MI, 4 ...| Josephine|\n|{Bridgeport, NJ, ...|       Art|\n+--------------------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Read multiple files with different schema\nmultiplefile_df_new = spark.read.json(['/FileStore/tables/customers.json','/FileStore/tables/customers_new.json'])\nmultiplefile_df_new.show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"029d776d-5c3b-43fd-92ea-57098881c737"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n|{New Orleans, LA,...|     James|      null|\n|{Brighton, MI, 4 ...| Josephine|      null|\n|{Bridgeport, NJ, ...|       Art|      null|\n+--------------------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+----------+----------+\n|             address|first_name| last_name|\n+--------------------+----------+----------+\n|{New Orleans, LA,...|     James|Butterburg|\n|{Brighton, MI, 4 ...| Josephine|   Darakjy|\n|{Bridgeport, NJ, ...|       Art|    Chemel|\n|{New Orleans, LA,...|     James|      null|\n|{Brighton, MI, 4 ...| Josephine|      null|\n|{Bridgeport, NJ, ...|       Art|      null|\n+--------------------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Read all JSON files from a folder\ndf3 = spark.read.json(\"resources/*.json\")\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36355503-717e-49f0-8846-7007e7e555eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb54b98c-b6a6-4da5-9653-f199d724c433"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f323056b-0724-40d7-a2c9-3278cb53e191"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n# One way to create a DataFrame is to first define an RDD from a list of Rows \nsome_rdd = sc.parallelize([Row(name=\"John\", age=19),\n                           Row(name=\"Smith\", age=23),\n                           Row(name=\"Sarah\", age=18)])\nsome_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27d05545-0ee0-4149-86c1-b3b45b2f9bb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[25]: [Row(age=19, name=&#39;John&#39;),\n Row(age=23, name=&#39;Smith&#39;),\n Row(age=18, name=&#39;Sarah&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[25]: [Row(age=19, name=&#39;John&#39;),\n Row(age=23, name=&#39;Smith&#39;),\n Row(age=18, name=&#39;Sarah&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# The DataFrame is created from the RDD or Rows\n# Infer schema from the first row, create a DataFrame and print the schema\nsome_df = spark.createDataFrame(some_rdd)\nsome_df.printSchema()\nsome_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e640db8f-5769-48c8-8daf-dc57c4b20434"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"some_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"age","nullable":true,"type":"long"},{"metadata":{},"name":"name","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n+---+-----+\n|age| name|\n+---+-----+\n| 19| John|\n| 23|Smith|\n| 18|Sarah|\n+---+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- age: long (nullable = true)\n-- name: string (nullable = true)\n\n+---+-----+\nage| name|\n+---+-----+\n 19| John|\n 23|Smith|\n 18|Sarah|\n+---+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# A dataframe is an RDD of rows plus information on the schema.\n# performing **collect()* on either the RDD or the DataFrame gives the same result.\nprint(type(some_rdd),type(some_df))\nsome_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a869c426-9574-4f09-b221-04220f5c5a65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.RDD&#39;&gt; &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+---+-----+\n|age| name|\n+---+-----+\n| 19| John|\n| 23|Smith|\n| 18|Sarah|\n+---+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.RDD&#39;&gt; &lt;class &#39;pyspark.sql.dataframe.DataFrame&#39;&gt;\n+---+-----+\nage| name|\n+---+-----+\n 19| John|\n 23|Smith|\n 18|Sarah|\n+---+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\nrdd = spark.sparkContext.parallelize(data)\ndfFromRDD1 = spark.createDataFrame(rdd)\ndfFromRDD1.printSchema()\ndfFromRDD1.show()\n\ndfFromRDD2 = rdd.toDF([\"language\",\"users_count\"])\n#dfFromRDD2.printSchema()\ndfFromRDD2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"402549bf-d780-487c-b21a-0940dc33f09c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"dfFromRDD1","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"_1","nullable":true,"type":"string"},{"metadata":{},"name":"_2","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"dfFromRDD2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"language","nullable":true,"type":"string"},{"metadata":{},"name":"users_count","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+------+------+\n|    _1|    _2|\n+------+------+\n|  Java| 20000|\n|Python|100000|\n| Scala|  3000|\n+------+------+\n\n+--------+-----------+\n|language|users_count|\n+--------+-----------+\n|    Java|      20000|\n|  Python|     100000|\n|   Scala|       3000|\n+--------+-----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _1: string (nullable = true)\n-- _2: long (nullable = true)\n\n+------+------+\n    _1|    _2|\n+------+------+\n  Java| 20000|\nPython|100000|\n Scala|  3000|\n+------+------+\n\n+--------+-----------+\nlanguage|users_count|\n+--------+-----------+\n    Java|      20000|\n  Python|     100000|\n   Scala|       3000|\n+--------+-----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["##data11 = [(\"Java\"),(\"Python\"),(\"CPP\")]\ndata11 = [(\"Java\",), (\"Python\",), (\"Scala\", )]\nrdd11 = spark.sparkContext.parallelize(data11)\ndfFromRDD11 = spark.createDataFrame(rdd11)\ndfFromRDD11.printSchema()\ndfFromRDD11.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4a2dea1-076d-4e26-b5df-25027d244d36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"dfFromRDD11","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"_1","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- _1: string (nullable = true)\n\n+------+\n|    _1|\n+------+\n|  Java|\n|Python|\n| Scala|\n+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _1: string (nullable = true)\n\n+------+\n    _1|\n+------+\n  Java|\nPython|\n Scala|\n+------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import LongType\n# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\nanother_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n# Schema with two fields - person_name and person_age\nschema = StructType([StructField(\"person_name\", StringType(), False),\n                     StructField(\"person_age\", IntegerType(), False)])\n\n# Create a DataFrame by applying the schema to the RDD and print the schema\nanother_df = sqlContext.createDataFrame(another_rdd, schema)\nanother_df.printSchema()\n# root\n#  |-- age: binteger (nullable = true)\n#  |-- name: string (nullable = true)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f441333-79d0-4b8a-99e7-5c5a7ac70145"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"another_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"person_name","nullable":false,"type":"string"},{"metadata":{},"name":"person_age","nullable":false,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- person_name: string (nullable = false)\n |-- person_age: integer (nullable = false)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- person_name: string (nullable = false)\n-- person_age: integer (nullable = false)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# when loading json files you can specify either a single file or a directory containing many json files.\npath = \"/FileStore/tables/people-1.json\"\n\n# Create a DataFrame from the file(s) pointed to by path\npeople_df = spark.read.json(path)\nprint('people is a',type(people_df))\n# The inferred schema can be visualized using the printSchema() method.\npeople_df.show()\n\npeople_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a67e741d-a805-42c5-a820-bdb0f7285c3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4318184040079885>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Create a DataFrame from the file(s) pointed to by path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mpeople_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'people is a'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpeople_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# The inferred schema can be visualized using the printSchema() method.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mjson\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: dbfs:/FileStore/tables/people-1.json","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Path does not exist: dbfs:/FileStore/tables/people-1.json","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4318184040079885>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Create a DataFrame from the file(s) pointed to by path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mpeople_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'people is a'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpeople_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# The inferred schema can be visualized using the printSchema() method.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mjson\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001B[0m\n\u001B[1;32m    227\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: dbfs:/FileStore/tables/people-1.json"]}}],"execution_count":0},{"cell_type":"code","source":["df2=people_df.select(\"name\").where(people_df['name']=='Andy')\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f93403e0-7b87-48b6-a409-f762a7a16562"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"name","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+----+\n|name|\n+----+\n|Andy|\n+----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\nname|\n+----+\nAndy|\n+----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df2.write.parquet(\"/FileStore/tables/output5th\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06350460-0953-4ce4-9c23-c1530f739357"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\n\nls -ltr /dbfs/FileStore/tables/output5th"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57f3743e-69c0-4130-8414-e90a1584fea1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ls: cannot access '/dbfs/FileStore/tables/output5th': No such file or directory\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ls: cannot access '/dbfs/FileStore/tables/output5th': No such file or directory\n"]}}],"execution_count":0},{"cell_type":"code","source":["# import pyspark class Row from module sql\nfrom pyspark.sql import *\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\nemployee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f44b8d7-a902-4bac-9b57-e286b47792cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n\ndisplay(df1)\ndf1.printSchema\ndepartmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\ndf2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n\ndisplay(df2)\ndf2.printSchema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee4a6666-c403-40cb-bf99-a9b3e7c846eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"df1","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"department","nullable":true,"type":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"name","nullable":true,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"employees","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"firstName","nullable":true,"type":"string"},{"metadata":{},"name":"lastName","nullable":true,"type":"string"},{"metadata":{},"name":"email","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"long"}],"type":"struct"},"type":"array"}}],"type":"struct"},"tableIdentifier":null},{"name":"df2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"department","nullable":true,"type":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"name","nullable":true,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"employees","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"firstName","nullable":true,"type":"string"},{"metadata":{},"name":"lastName","nullable":true,"type":"string"},{"metadata":{},"name":"email","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"long"}],"type":"struct"},"type":"array"}}],"type":"struct"},"tableIdentifier":null}],"data":[[["345678","Theater and Drama"],[["michael","jackson","no-reply@neverla.nd",80000],[null,"wendell","no-reply@berkeley.edu",160000]]],[["901234","Indoor Recreation"],[["xiangrui","meng","no-reply@stanford.edu",120000],["matei",null,"no-reply@waterloo.edu",140000]]]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"department","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"employees","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"firstName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"email\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employees</th></tr></thead><tbody><tr><td>List(345678, Theater and Drama)</td><td>List(List(michael, jackson, no-reply@neverla.nd, 80000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr><tr><td>List(901234, Indoor Recreation)</td><td>List(List(xiangrui, meng, no-reply@stanford.edu, 120000), List(matei, null, no-reply@waterloo.edu, 140000))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df2.select(\"department.name\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9ea108e-1aa0-4315-957b-df50f46e62e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+\n|             name|\n+-----------------+\n|Theater and Drama|\n|Indoor Recreation|\n+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+\n             name|\n+-----------------+\nTheater and Drama|\nIndoor Recreation|\n+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["unionDF = df1.union(df2)\ndisplay(unionDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"378adde5-aa36-41c9-90fe-6dac4cdd633c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"unionDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"department","nullable":true,"type":{"fields":[{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"name","nullable":true,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"employees","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"firstName","nullable":true,"type":"string"},{"metadata":{},"name":"lastName","nullable":true,"type":"string"},{"metadata":{},"name":"email","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"long"}],"type":"struct"},"type":"array"}}],"type":"struct"},"tableIdentifier":null}],"data":[[["123456","Computer Science"],[["michael","armbrust","no-reply@berkeley.edu",100000],["xiangrui","meng","no-reply@stanford.edu",120000]]],[["789012","Mechanical Engineering"],[["matei",null,"no-reply@waterloo.edu",140000],[null,"wendell","no-reply@berkeley.edu",160000]]],[["345678","Theater and Drama"],[["michael","jackson","no-reply@neverla.nd",80000],[null,"wendell","no-reply@berkeley.edu",160000]]],[["901234","Indoor Recreation"],[["xiangrui","meng","no-reply@stanford.edu",120000],["matei",null,"no-reply@waterloo.edu",140000]]]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"department","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"employees","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"firstName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"email\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"salary\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>employees</th></tr></thead><tbody><tr><td>List(123456, Computer Science)</td><td>List(List(michael, armbrust, no-reply@berkeley.edu, 100000), List(xiangrui, meng, no-reply@stanford.edu, 120000))</td></tr><tr><td>List(789012, Mechanical Engineering)</td><td>List(List(matei, null, no-reply@waterloo.edu, 140000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr><tr><td>List(345678, Theater and Drama)</td><td>List(List(michael, jackson, no-reply@neverla.nd, 80000), List(null, wendell, no-reply@berkeley.edu, 160000))</td></tr><tr><td>List(901234, Indoor Recreation)</td><td>List(List(xiangrui, meng, no-reply@stanford.edu, 120000), List(matei, null, no-reply@waterloo.edu, 140000))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\nexplodeDF = unionDF.select(explode(\"employees.firstName\"))\nexplodeDF.show(2,truncate= False)\nunexplodeDF = unionDF.select(\"employees.firstName\")\nunexplodeDF.show(2,truncate= False)\n#unionDF.printSchema()\n#explodeDF.printSchema()\n#unexplodeDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eed4a330-669c-417b-a135-97b1c3da5e50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"explodeDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"col","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"unexplodeDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"firstName","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------+\n|col     |\n+--------+\n|michael |\n|xiangrui|\n+--------+\nonly showing top 2 rows\n\n+-------------------+\n|firstName          |\n+-------------------+\n|[michael, xiangrui]|\n|[matei,]           |\n+-------------------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncol     |\n+--------+\nmichael |\nxiangrui|\n+--------+\nonly showing top 2 rows\n\n+-------------------+\nfirstName          |\n+-------------------+\n[michael, xiangrui]|\n[matei,]           |\n+-------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["filterDF = flattenDF.select(\"email\").filter(flattenDF.firstName == \"xiangrui\").sort(flattenDF.lastName)\ndisplay(filterDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"755b7657-ab59-4ea3-8814-505d9c541a2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"filterDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"email","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["no-reply@stanford.edu"],["no-reply@stanford.edu"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"email","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>email</th></tr></thead><tbody><tr><td>no-reply@stanford.edu</td></tr><tr><td>no-reply@stanford.edu</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\nwhereDF = flattenDF.select(\"email\").where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(whereDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"276ce8d4-3814-459a-a7fa-001b48f04e56"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"whereDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"email","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["no-reply@berkeley.edu"],["no-reply@neverla.nd"],["no-reply@stanford.edu"],["no-reply@stanford.edu"]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"email","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>email</th></tr></thead><tbody><tr><td>no-reply@berkeley.edu</td></tr><tr><td>no-reply@neverla.nd</td></tr><tr><td>no-reply@stanford.edu</td></tr><tr><td>no-reply@stanford.edu</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\n\ncountDistinctDF = whereDF.select(\"firstName\", \"lastName\")\\\n  .groupBy(\"firstName\")\\\n  .agg(countDistinct(\"lastName\").alias(\"distinct_last_names\"))\n\ndisplay(countDistinctDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8364c04c-95bd-4afd-99b2-4e7351dc7123"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"countDistinctDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"firstName","nullable":true,"type":"string"},{"metadata":{},"name":"distinct_last_names","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":[["xiangrui",1],["michael",2]],"plotOptions":{"displayType":"scatterPlot","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"firstName","type":"\"string\"","metadata":"{}"},{"name":"distinct_last_names","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>firstName</th><th>distinct_last_names</th></tr></thead><tbody><tr><td>xiangrui</td><td>1</td></tr><tr><td>michael</td><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\n%fs ls /FileStore/tables/item_data.csv\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35e7645f-d9da-497a-9b69-f4eb55f555ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/FileStore/tables/item_data.csv","item_data.csv",161,1651191893000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/item_data.csv</td><td>item_data.csv</td><td>161</td><td>1651191893000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["with open(\"/dbfs/foobar/item_data.csv\") as f:\n  for line in f_read:\n    print(line)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dee3df0c-468d-413a-a55f-dc0279b0eb03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2262737241941782>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/dbfs/foobar/item_data.csv\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m   \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf_read\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/dbfs/foobar/item_data.csv'","errorSummary":"<span class='ansi-red-fg'>FileNotFoundError</span>: [Errno 2] No such file or directory: '/dbfs/foobar/item_data.csv'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2262737241941782>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/dbfs/foobar/item_data.csv\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m   \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf_read\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/dbfs/foobar/item_data.csv'"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\n\nls -ltr /dbfs/FileStore/tables/people.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd5fada3-fdc1-43d9-9372-bef2d3b23e42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ls: cannot access '/dbfs/FileStore/tables/people.txt': No such file or directory\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ls: cannot access '/dbfs/FileStore/tables/people.txt': No such file or directory\n"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/foobar/\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"529a9cd9-5609-4afa-91ed-5476bb80efbf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[24]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[24]: True"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"/FileStore/tables/item_data.csv\",\"/foobar/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6af7bbdb-6b30-48f4-8db6-c70fc61b4737"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[25]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[25]: True"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv('dbfs:/foobar/item_data.csv')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0554e2ca-259f-4c9b-9ef7-37c47bca0f84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------+\n|_c0|                 _c1|\n+---+--------------------+\n| 12|         Coconut Oil|\n| 13|          peanut oil|\n| 14|           olive oil|\n| 15|          almond oil|\n| 16|Virgin-organic Co...|\n| 17|Pure Virgin Organ...|\n| 18|          sesame oil|\n+---+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------+\n|_c0|                 _c1|\n+---+--------------------+\n| 12|         Coconut Oil|\n| 13|          peanut oil|\n| 14|           olive oil|\n| 15|          almond oil|\n| 16|Virgin-organic Co...|\n| 17|Pure Virgin Organ...|\n| 18|          sesame oil|\n+---+--------------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2 Dataframe (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4318184040079878}},"nbformat":4,"nbformat_minor":0}
